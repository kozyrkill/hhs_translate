import os
import re
import time
import json
import argparse
import statistics
from lxml import etree as ET
from langdetect import detect_langs, DetectorFactory
from deep_translator import GoogleTranslator
from functools import lru_cache

DetectorFactory.seed = 0

BB_RE = re.compile(r"(\[/?[^\]]+\])")
PLACEHOLDER_RE = re.compile(r"{[^{}]+}")
CACHE_FILE = "translate_cache.json"

def detect_bbcode(text: str) -> bool:
    return '[' in text and ']' in text

class GoogleTranslateWrapper:
    def __init__(self):
        self.translator = GoogleTranslator(source='en', target='ru')

    def translate(self, text: str) -> str:
        return self.translator.translate(text)

    def translate_batch(self, texts):
        return [self.translate(t) for t in texts]

def mask_placeholders(text: str):
    placeholders = []
    def replace_func(match):
        idx = len(placeholders)
        placeholders.append(match.group(0))
        return f"[[PH{idx}]]"
    masked = re.sub(PLACEHOLDER_RE, replace_func, text)
    return masked, placeholders

def unmask_placeholders(text: str, placeholders):
    for i, ph in enumerate(placeholders):
        pattern = re.compile(re.escape(f"[[PH{i}]]"), re.IGNORECASE)
        text = pattern.sub(ph, text)
    return text

def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            print("‚ö†Ô∏è –ü–æ–≤—Ä–µ–∂–¥—ë–Ω–Ω—ã–π –∫—ç—à ‚Äî –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º translate_cache.json")
            return {}
    return {}

def save_cache_with_source(cache, source_map):
    enriched_cache = {}
    for k, v in cache.items():
        if isinstance(v, dict):
            # –£–∂–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {"text": "...", "source": "..."}
            enriched_cache[k] = v
        else:
            enriched_cache[k] = {
                "text": v,
                "source": source_map.get(k, "unknown")
            }
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(enriched_cache, f, ensure_ascii=False, indent=2)

def is_probably_english(text: str, threshold=0.9) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Å –±–æ–ª—å—à–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –∞–Ω–≥–ª–∏–π—Å–∫–∏–º.
    """
    try:
        langs = detect_langs(text)
        for langprob in langs:
            if langprob.lang == 'en' and langprob.prob >= threshold:
                return True
        return False
    except:
        return False

def translate_bbcode_preserving_tags(text: str, translator):
    parts = BB_RE.split(text)
    translated_parts = []
    for part in parts:
        if BB_RE.match(part):
            translated_parts.append(part)  # BB-—Ç–µ–≥
        else:
            lines = part.split('\n')
            translated_lines = []
            for line in lines:
                line = line.strip()
                if line:
                    try:
                        masked, placeholders = mask_placeholders(line)
                        translated_line = translator.translate(masked) or ''
                        translated_line = unmask_placeholders(translated_line, placeholders)
                        translated_lines.append(translated_line)
                    except Exception as e:
                        print(f"[!] –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ BB-–∫–æ–¥–∞ —Å—Ç—Ä–æ–∫–∏: {line} ‚Äî {e}")
                        translated_lines.append(line)
                else:
                    translated_lines.append('')
            translated_parts.append('\n'.join(translated_lines))
    return ''.join(translated_parts)

def batch_translate(texts, translator, delay_seconds=1.5, max_chars_per_batch=3000, sources=None):
    """
    –ü–µ—Ä–µ–≤–æ–¥–∏–º —Å–ø–∏—Å–∫–æ–º (batched). –ü–∏—à–µ–º –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à.
    –ü–∞—Ä–∞–º–µ—Ç—Ä `sources` ‚Äî —Å–ø–∏—Å–æ–∫ —Ç–∞–∫–æ–π –∂–µ –¥–ª–∏–Ω—ã, —á—Ç–æ texts, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ "source".
    """
    cache = load_cache()
    source_map = {}
    results = []
    to_translate = []
    masked_texts = []
    all_placeholders = []
    original_map = []

    for idx, text in enumerate(texts):
        if isinstance(cache.get(text), dict):
            results.append(cache[text]["text"])
        elif text in cache:
            results.append(cache[text])
        else:
            results.append(None)
            to_translate.append(text)
            masked, placeholders = mask_placeholders(text)
            masked_texts.append(masked)
            all_placeholders.append(placeholders)

            field = sources[idx] if sources and idx < len(sources) else "unknown"
            original_map.append((len(results) - 1, field))

    if not to_translate:
        print("‚ö° –í—Å—ë –Ω–∞–π–¥–µ–Ω–æ –≤ –∫—ç—à–µ.")
        return results

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    batch_queue = []
    batch = []
    batch_len = 0

    for i, masked_text in enumerate(masked_texts):
        text_len = len(masked_text)
        
        # –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è ‚Äî –æ—Ç–ø—Ä–∞–≤–∏–º –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –±–∞—Ç—á
        if text_len > max_chars_per_batch:
            if batch:
                batch_queue.append(batch)
                batch = []
                batch_len = 0
            batch_queue.append([masked_text])
            continue

        if batch_len + text_len > max_chars_per_batch:
            batch_queue.append(batch)
            batch = []
            batch_len = 0

        batch.append(masked_text)
        batch_len += text_len

    if batch:
        batch_queue.append(batch)

    total_batches = len(batch_queue)
    print(f"üîÑ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–æ: {len(texts) - len(to_translate)}, –Ω–∞ –ø–µ—Ä–µ–≤–æ–¥: {len(to_translate)}")
    print(f"üîÑ –í—Å–µ–≥–æ –±–∞—Ç—á–µ–π: {total_batches} (–¥–æ {max_chars_per_batch} —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∂–¥—ã–π)")

    import statistics
    batch_times = []
    dynamic_delay = delay_seconds
    max_delay = 60
    min_delay = 1.0
    translated_all = []
    start_idx = 0

    for i, batch_part in enumerate(batch_queue, start=1):
        print(f"‚û°Ô∏è –ë–∞—Ç—á {i} –∏–∑ {total_batches}‚Ä¶")
        start_time = time.time()
        success = False
        retries = 0

        while not success and retries < 5:
            try:
                translated = translator.translate_batch(batch_part)
                for j, translated_text in enumerate(translated):
                    placeholders = all_placeholders[start_idx + j]
                    translated[j] = unmask_placeholders(translated_text, placeholders)

                translated_all.extend(translated)

                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –∫—ç—à
                for j, src_text in enumerate(to_translate[start_idx:start_idx+len(translated)]):
                    _, source_field = original_map[start_idx + j]
                    cache[src_text] = translated[j]
                    source_map[src_text] = source_field

                save_cache_with_source(cache, source_map)
                success = True
                dynamic_delay = max(min_delay, dynamic_delay * 0.9)

            except Exception as e:
                print(f"[!] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ –±–∞—Ç—á–∞ {i}: {e}")
                retries += 1
                dynamic_delay = min(max_delay, dynamic_delay * 1.5)
                wait_time = int(dynamic_delay)
                print(f"‚è≥ –ñ–¥—ë–º {wait_time} —Å–µ–∫. –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º (–ø–æ–ø—ã—Ç–∫–∞ {retries})‚Ä¶")
                time.sleep(wait_time)

        if not success:
            raise RuntimeError(f"‚ùå –ü–µ—Ä–µ–≤–æ–¥ –±–∞—Ç—á–∞ {i} –Ω–µ —É–¥–∞–ª—Å—è –ø–æ—Å–ª–µ 5 –ø–æ–ø—ã—Ç–æ–∫.")

        duration = time.time() - start_time
        batch_times.append(duration)

        if success and i < total_batches:
            avg = statistics.mean(batch_times)
            remaining = total_batches - i
            eta = avg * remaining + dynamic_delay * remaining
            print(f"‚è≥ –û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–º–µ—Ä–Ω–æ: {int(eta)} —Å–µ–∫. (–µ—â—ë {remaining} –±–∞—Ç—á–µ–π)")

        start_idx += len(batch_part)
        time.sleep(dynamic_delay)

    for idx, text in enumerate(texts):
        if results[idx] is None:
            if isinstance(cache.get(text), dict):
                results[idx] = cache[text]["text"]
            else:
                results[idx] = cache[text]

    print("‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à—ë–Ω (—Å –∫—ç—à–µ–º).")
    return results


##################
#   –í–ê–ñ–ù–ê–Ø –ß–ê–°–¢–¨ #
##################

def find_parent_elements(root, parent_tag: str):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ <parent_tag> –≤–Ω—É—Ç—Ä–∏ root (—Ç. –µ. .//parent_tag),
    –ø–ª—é—Å, –µ—Å–ª–∏ –∫–æ—Ä–µ–Ω—å —Å–∞–º –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è parent_tag, —Ç–æ–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º root.
    """
    matched = root.findall(f".//{parent_tag}")
    if root.tag == parent_tag:
        matched.insert(0, root)
    return matched

def parse_field_with_condition(field_str: str):
    """
    –†–∞–∑–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫—É –≤–∏–¥–∞: "SeqVar_String::Str?UserFacing=True"
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º (parent_tag, child_tag, cond_tag, cond_val, is_parent_child).
    """
    condition_tag = None
    condition_val = None
    parent_tag = None
    child_tag = None
    is_parent_child = False

    # –°–Ω–∞—á–∞–ª–∞ –æ—Ç–¥–µ–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω–æ–µ "?cond=val"
    if "?" in field_str:
        main_part, cond_part = field_str.split("?", 1)
    else:
        main_part = field_str
        cond_part = None

    # –ï—Å–ª–∏ –µ—Å—Ç—å cond_part –≤—Ä–æ–¥–µ "UserFacing=True"
    if cond_part and "=" in cond_part:
        condition_tag, condition_val = cond_part.split("=", 1)
        condition_tag = condition_tag.strip()
        condition_val = condition_val.strip()

    # –¢–µ–ø–µ—Ä—å –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ "::"
    if "::" in main_part:
        parent_tag, child_tag = main_part.split("::", 1)
        parent_tag = parent_tag.strip()
        child_tag = child_tag.strip()
        is_parent_child = True
    else:
        parent_tag = main_part.strip()

    return parent_tag, child_tag, condition_tag, condition_val, is_parent_child

def check_parent_condition(parent_elem: ET.Element, condition_tag: str, condition_val: str) -> bool:
    """
    –ï—Å–ª–∏ condition_tag –Ω–µ –∑–∞–¥–∞–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º True (–Ω–µ—Ç —É—Å–ª–æ–≤–∏—è).
    –ò–Ω–∞—á–µ –∏—â–µ–º <condition_tag> –≤–Ω—É—Ç—Ä–∏ parent_elem (.find(condition_tag)).
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º .text.strip() == condition_val.
    """
    if not condition_tag:
        return True
    found = parent_elem.find(condition_tag)
    if found is not None and found.text and found.text.strip() == condition_val:
        return True
    return False

def translate_fields(root_folder, fields, delay_seconds=1.5, max_chars_per_batch=3000, strict=False):
    """
    –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:
    - fields –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–∞–∫ "Description" (–æ–¥–∏–Ω–æ—á–Ω—ã–π —Ç–µ–≥),
      —Ç–∞–∫ –∏ "SeqVar_String::Str?UserFacing=True" (—Ä–æ–¥–∏—Ç–µ–ª—å::—Ä–µ–±—ë–Ω–æ–∫ + —É—Å–ª–æ–≤–∏–µ).
    - –ï—Å–ª–∏ –µ—Å—Ç—å —É—Å–ª–æ–≤–∏–µ "?UserFacing=True", —Ç–æ –ø–µ—Ä–µ–¥ —Ç–µ–º, –∫–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å <Str>,
      –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–Ω—É—Ç—Ä–∏ —Ä–æ–¥–∏—Ç–µ–ª—è <UserFacing> —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º "True".
    """
    translator = GoogleTranslateWrapper()
    parser = ET.XMLParser(remove_blank_text=True, recover=not strict)

    texts_to_translate = []
    sources_for_translate = []
    text_locations = []
    modified_files = {}
    error_log = []

    for dirpath, _, filenames in os.walk(root_folder):
        for filename in filenames:
            if not filename.lower().endswith('.xml'):
                continue
            file_path = os.path.join(dirpath, filename)

            try:
                tree = ET.parse(file_path, parser)
                root = tree.getroot()
                modified_files[file_path] = (tree, False)

                # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è-—É—Å–ª–æ–≤–∏—è
                for field_def in fields:
                    parent_tag, child_tag, cond_tag, cond_val, is_parent_child = parse_field_with_condition(field_def)

                    if is_parent_child:
                        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤—Å–µ—Ö parent_tag
                        parent_elems = find_parent_elements(root, parent_tag)
                        for p_elem in parent_elems:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏–µ (cond_tag=UserFacing? cond_val=?)
                            if not check_parent_condition(p_elem, cond_tag, cond_val):
                                continue
                            # –ï—Å–ª–∏ —É—Å–ª–æ–≤–∏–µ –ø—Ä–æ—Ö–æ–¥–∏—Ç, –∏—â–µ–º –≤–Ω—É—Ç—Ä–∏ p_elem -> child_tag
                            child_elems = p_elem.findall(f".//{child_tag}")
                            for elem in child_elems:
                                if not elem.text:
                                    continue
                                txt = elem.text.strip()
                                if is_probably_english(txt):
                                    # –°–æ–±–∏—Ä–∞–µ–º
                                    texts_to_translate.append(txt)
                                    # –î–ª—è –∫—ç—à–∞: –ø–∏—à–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å + –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª–µ–π
                                    src_info = f"{file_path}:{parent_tag}::{child_tag}"
                                    if cond_tag and cond_val:
                                        src_info += f"?{cond_tag}={cond_val}"
                                    sources_for_translate.append(src_info)
                                    text_locations.append((elem, file_path))
                    else:
                        # –û–¥–∏–Ω–æ—á–Ω—ã–π —Ç–µ–≥
                        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º .//parent_tag
                        found_elems = root.findall(f".//{parent_tag}")
                        if root.tag == parent_tag:
                            found_elems.insert(0, root)
                        for elem in found_elems:
                            # –ø—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏–µ –∫ —Å–∞–º–æ–º—É elem
                            if not check_parent_condition(elem, cond_tag, cond_val):
                                continue
                            if not elem.text:
                                continue
                            txt = elem.text.strip()
                            if is_probably_english(txt):
                                texts_to_translate.append(txt)
                                src_info = f"{file_path}:{parent_tag}"
                                if cond_tag and cond_val:
                                    src_info += f"?{cond_tag}={cond_val}"
                                sources_for_translate.append(src_info)
                                text_locations.append((elem, file_path))

            except Exception as e:
                msg = f"[!] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path}: {e}"
                print(msg)
                error_log.append(msg)

    if not texts_to_translate:
        print("üéâ –í—Å—ë —É–∂–µ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–æ –∏–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return

    print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞: {len(texts_to_translate)}")
    translated_texts = batch_translate(
        texts_to_translate,
        translator,
        delay_seconds=delay_seconds,
        max_chars_per_batch=max_chars_per_batch,
        sources=sources_for_translate
    )

    # –†–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã –æ–±—Ä–∞—Ç–Ω–æ
    for (elem, file_path), new_text in zip(text_locations, translated_texts):
        # –ï—Å–ª–∏ —É –≤–∞—Å bbcode, –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –¥–æ–ø. –ø—Ä–æ–≤–µ—Ä–∫—É:
        # if detect_bbcode(elem.text or ""):
        #     elem.text = translate_bbcode_preserving_tags(elem.text, translator)
        # else:
        #     elem.text = new_text
        elem.text = new_text
        modified_files[file_path] = (modified_files[file_path][0], True)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    for file_path, (tree, was_modified) in modified_files.items():
        if was_modified:
            try:
                tree.write(file_path, encoding='utf-8', xml_declaration=True, pretty_print=True)
                print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {file_path}")
            except Exception as e:
                msg = f"[!] –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {file_path}: {e}"
                print(msg)
                error_log.append(msg)

    if error_log:
        with open("log_errors.txt", "w", encoding="utf-8") as f:
            f.write("\n".join(error_log))
        print("\n‚ö†Ô∏è –û—à–∏–±–∫–∏ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ log_errors.txt")

def main():
    parser = argparse.ArgumentParser(description="–ü–µ—Ä–µ–≤–æ–¥ XML (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Parent::Child?Tag=Value)")
    parser.add_argument("--path", required=True, help="–ü–∞–ø–∫–∞ —Å .xml —Ñ–∞–π–ª–∞–º–∏")
    parser.add_argument("--fields", nargs="+", default=["SeqVar_String::Str?UserFacing=True"],
                        help="–ü–æ–ª—è –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞, –Ω–∞–ø—Ä: SeqVar_String::Str?UserFacing=True Description")
    parser.add_argument("--strict", action="store_true", help="–û—Ç–∫–ª—é—á–∏—Ç—å recover –≤ XML-–ø–∞—Ä—Å–∏–Ω–≥–µ")
    parser.add_argument("--delay", type=float, default=1.5, help="–ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏")
    parser.add_argument("--maxchars", type=int, default=3000, help="–ú–∞–∫—Å. —Å–∏–º–≤–æ–ª–æ–≤ –≤ –±–∞—Ç—á–µ")
    args = parser.parse_args()

    translate_fields(
        root_folder=args.path,
        fields=args.fields,
        delay_seconds=args.delay,
        max_chars_per_batch=args.maxchars,
        strict=args.strict
    )

if __name__ == "__main__":
    main()
